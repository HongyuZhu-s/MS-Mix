<div align="center">
<!-- <h1>MS-Mix: An Adaptive Emotion-Sensitive Mixup Framework for Multimodal Sentiment Analysis </h1> -->
<h2>MS-Mix: An Adaptive Emotion-Sensitive Mixup Framework for Multimodal Sentiment Analysis </a></h2>

<p align="center">
<img src="https://github.com/HongyuZhu-s/MS-Mix/blob/main/Figs/8894c352-5b03-4db7-ba93-23277e89e530.png" width=75% height=75% 
class="center">
</p>

<div align="left">
Although Mixup-based augmentation improves generalization in unimodal tasks, its direct application to MSA introduces critical challenges: random mixing often amplifies label ambiguity and semantic inconsistency due to the lack of emotion-aware mixing mechanisms. To overcome these issues, we propose **MS-Mix**, an adaptive, emotion-sensitive augmentation framework that automatically optimizes sample mixing in multimodal settings.
  

## 1. Installation
We use the **M-SENA framework** ("https://github.com/thuiar/MMSA") to evaluate MSA models. It is a unified framework for multimodal sentiment analysis. Please install it before proceeding.

*Run `pip install MMSA` in your python virtual environment.*

## 2. Dataset
We conducted extensive experiments on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets. You can download raw videos, feature files, and label files via the following links: [Dataset](https://drive.google.com/drive/folders/1A2S4pqCHryGmiqnNSPLv7rEg63WvjCSk?usp=sharing)

___
### If you are interested in my work, you can contact me by email: zhuhongyu@cigit.ac.cn
